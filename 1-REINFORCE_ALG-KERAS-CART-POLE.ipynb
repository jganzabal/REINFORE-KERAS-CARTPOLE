{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de librería gym\n",
    "\n",
    "https://gym.openai.com/\n",
    "\n",
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt_text](reinforcement_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge max_{\\theta} \\quad\\mathbb{E_{\\pi_{\\theta}}}\\{\\sum_{t=0}^{T}R(s_t, a_t)\\}$$\n",
    "\n",
    "Donde R es la recompensa que se recibirá luego de la acción $a_t$ estando en el estado $s_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge R(\\tau) = \\sum_{t=0}^{T}R(s_t, a_t)\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge J_{\\theta} = \\sum_{\\tau} P(\\tau | \\theta) R(\\tau)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\tau, \\theta)$ es la probabilidad de una trayectoria dada la policy y contiene el modelo $P(s_i|a_i, s_{i-1})$ y también la policy $\\pi_{\\theta}(a_i|s_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge P(\\tau| \\theta) = P(s_0)\\prod_{t=0}^T P(s_{t+1}|a_t, s_{t}) \\pi_{\\theta}(a_t|s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Markov_Decision_Process.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomando el gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge  \\nabla_{\\theta} J_{\\theta} =  \\sum_{\\tau} \\nabla_{\\theta} P(\\tau| \\theta) R(\\tau)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge \\nabla_{\\theta} J_{\\theta} =  \\sum_{\\tau} \\frac{  P(\\tau| \\theta)  \\nabla_{\\theta}  P(\\tau| \\theta) R(\\tau)}{P(\\tau, \\theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge \\nabla_{\\theta} J_{\\theta} =   \\sum_{\\tau} P(\\tau| \\theta)   \\nabla_{\\theta} log P(\\tau| \\theta) R(\\tau)$$\n",
    "\n",
    "El gradiente de la policy es una esperanza:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge \\nabla_{\\theta} J_{\\theta} =   \\mathbb{E_{\\tau}} \\{\\nabla_{\\theta} log P(\\tau| \\theta) R(\\tau)\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montecarlo, unbiased, alta varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge \\nabla_{\\theta} J_{\\theta} = \\frac{1}{M} \\sum_{i=1}^{M} \\nabla_{\\theta} log P(\\tau_i| \\theta) R(\\tau_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge \\nabla_{\\theta} J_{\\theta} =  \\frac{1}{M} \\sum_{i=1}^{M} R(\\tau_i) \\nabla_{\\theta}  log [\\prod_{t=0}^T P(s_{t+1}^i|a_t^i, s_{t}^i) \\pi_{\\theta}(a_t^i|s_t^i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge  \\nabla_{\\theta} J_{\\theta} =  \\frac{1}{M}  \\sum_{i=1}^{M} R(\\tau_i) \\{\\sum_{t=0}^T \\nabla_{\\theta} log P(s_{t+1}^i|a_t^i, s_{t}^i) + \\sum_{t=0}^T \\nabla_{\\theta} log \\pi_{\\theta}(a_t^i|s_t^i) \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\huge \\nabla_{\\theta} J_{\\theta} = \\frac{1}{M} \\sum_{i=1}^{M}  R(\\tau_i)   \\sum_{t=0}^T \\nabla_{\\theta} log \\pi_{\\theta}(a_t^i|s_t^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\huge J_{\\theta} = \\frac{1}{M} \\sum_{i=1}^{M}  R(\\tau_i) \\sum_{t=0}^T  log \\pi_{\\theta}(a_t^i|s_t^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un estado $s_t$ entro a la red neuronal y da como resultado de la **softmax**, las probabilidades de las acciones:\n",
    "\n",
    "[0.1, 0.3, 0.6]\n",
    "\n",
    "y un reward acumulado de 58\n",
    "\n",
    "Puedo muestrear o hacer argmax. Supongo lo segundo por lo que puedo plantear que el label correcto (pseudolabel) es [0, 0, 1]\n",
    "\n",
    "Si aplico **categorical crossentropy**:\n",
    "\n",
    "J = 0xlog 0.1 + 0xlog 0.3 + 1xlog 0.6\n",
    "\n",
    "Si re defino el label como R * label: [0, 0, 1]*58\n",
    "\n",
    "J = 58x0xlog0.1 + 58x0xlog 0.3 + 58x1xlog 0.6\n",
    "\n",
    "De esta forma podemos resolver el problema con una red neuronal con keras sin demasiado trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole problem\n",
    "\n",
    "![alt text](cart_pole.gif \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE. Algoritmo\n",
    "\n",
    "## 1. Inicializar red neuronal con pesos aleatorios: $\\large \\pi_{\\theta}(a_t|s_t)$\n",
    "## 2. Correr un episodio y guardar estados ($s_t$), acciones ($a_t$) y rewards ($r_t$)\n",
    "## 3. Calcular la suma de los discounted rewards:\n",
    "$$ \\huge G_t = \\sum_{t'=t+1}^T \\gamma^{t'-t-1} r_{t'}$$\n",
    "## 4. Calcular el gradiente:\n",
    "$$\\huge \\nabla_{\\theta}J(\\theta) = \\sum_{t=0}^{T-1}{\\nabla_{\\theta} log \\pi_{\\theta}}(a_t|s_t)G_t$$\n",
    "## 5. Tomar un paso en dirección del gradiente (Queremos maximizar)\n",
    "$$ \\huge \\theta = \\theta + \\alpha \\nabla_{\\theta}J(\\theta) $$\n",
    "## 6. Repetir desde el paso 2 hasta convergencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[1., 0.], [0., 1.], [0, 1], [1, 0]]\n",
    "pi_outs = [[.9, .1], [.4, .6], [0.2, 0.8], [0.3, 0.7]]\n",
    "discounted_rewards = [12.24, 11.36, 10.466, 9.56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "actions = tf.constant(actions, shape=[4,2])\n",
    "pi = tf.constant(pi_outs, shape=[4,2])\n",
    "discounted_rewards = tf.constant(discounted_rewards, shape=[4,1])\n",
    "product = discounted_rewards*actions\n",
    "loss = K.categorical_crossentropy(product, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[0.9 0.1]\n",
      " [0.4 0.6]\n",
      " [0.2 0.8]\n",
      " [0.3 0.7]]\n",
      "[[12.24 ]\n",
      " [11.36 ]\n",
      " [10.466]\n",
      " [ 9.56 ]]\n",
      "[[12.24   0.   ]\n",
      " [ 0.    11.36 ]\n",
      " [ 0.    10.466]\n",
      " [ 9.56   0.   ]]\n",
      "[ 1.289613   5.8029785  2.3354201 11.50998  ]\n"
     ]
    }
   ],
   "source": [
    "actions_np = K.eval(actions)\n",
    "pi_np = K.eval(pi)\n",
    "discounted_rewards_np = K.eval(discounted_rewards)\n",
    "product_np = K.eval(product)\n",
    "loss_np = K.eval(loss)\n",
    "print(actions_np)\n",
    "print(pi_np)\n",
    "print(discounted_rewards_np)\n",
    "print(product_np)\n",
    "print(loss_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2896127116517937,\n",
       " -5.802979085981654,\n",
       " -2.3354204080545187,\n",
       " -11.50998000935595)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.9)*12.24, np.log(0.6)*11.36, np.log(0.8)*10.466, np.log(0.3)*9.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from reinforce_alg_helper import plot_episode, actions_to_one_hot, get_policy_model, apply_baselines, discount_rewards, score_model, run_episode, get_observations_stats, get_random_episode, get_batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.losses import categorical_crossentropy, binary_crossentropy\n",
    "\n",
    "def get_policy_model_softmax(lr=0.1, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "    model.compile(Adam(lr), loss=['categorical_crossentropy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/julianganzabal/anaconda3/envs/mllab/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_policy_model_softmax()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_with_policy(env, model, exploit = False):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    predictions = []\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    episode_length = 0\n",
    "    while not done:\n",
    "        (position_of_cart, velocity_of_cart, angle_of_pole, rotation_rate_of_pole) = observation\n",
    "        state = observation.reshape(1,-1)\n",
    "        states.append(observation)\n",
    "        predict = model.predict([state])[0]\n",
    "        predictions.append(predict)\n",
    "        if exploit:\n",
    "            #exploit\n",
    "            action = np.argmax(predict)\n",
    "        else:\n",
    "            #Explore\n",
    "            action = np.random.choice(range(len(predict)),p=predict)\n",
    "        actions.append(action)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        reward = (env.observation_space.high[2]/4 - np.abs(angle_of_pole))*180/np.pi\n",
    "        if done: reward-=12\n",
    "        rewards.append(reward)\n",
    "        episode_length += 1\n",
    "    return np.array(states), np.array(actions), np.array(rewards), episode_length, np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, episode_length, predictions = run_episode_with_policy(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_rewards(r, gamma = 0.999):\n",
    "    # Por si es una lista\n",
    "    r = np.array(r, dtype=float)\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.42213879e+00,  4.37920120e+00,  4.66155295e+00,  4.59844579e+00,\n",
       "        4.86217896e+00,  5.45234633e+00,  5.70002917e+00,  5.60888463e+00,\n",
       "        5.85123192e+00,  5.75573076e+00,  5.32390222e+00,  5.22591975e+00,\n",
       "        4.78830805e+00,  4.68117008e+00,  4.23108321e+00,  3.43739312e+00,\n",
       "        2.96777483e+00,  2.14700067e+00,  1.64200252e+00,  7.78217844e-01,\n",
       "       -4.47235577e-01, -1.37160477e+00, -2.00345109e+00, -2.34934748e+00,\n",
       "       -3.07714387e+00, -3.52593852e+00, -3.70104144e+00, -3.60574914e+00,\n",
       "       -3.24134391e+00, -2.60710410e+00, -2.36114953e+00, -1.83813375e+00,\n",
       "       -1.03624789e+00, -6.15295582e-01,  9.39428425e-02,  4.28809964e-01,\n",
       "        3.93166430e-01, -1.11285708e-02, -1.17150313e-01,  7.22760324e-02,\n",
       "        5.56415190e-01,  6.69939046e-01,  4.15530695e-01, -2.06163337e-01,\n",
       "       -1.19653580e+00, -2.55896568e+00, -4.29870807e+00, -1.77604859e+01])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6014472 , 0.3985528 ],\n",
       "       [0.40594658, 0.5940534 ],\n",
       "       [0.6037375 , 0.39626253],\n",
       "       [0.4094982 , 0.59050184],\n",
       "       [0.22768736, 0.77231264],\n",
       "       [0.38935626, 0.61064374],\n",
       "       [0.5860581 , 0.41394192],\n",
       "       [0.38615194, 0.61384803],\n",
       "       [0.5826964 , 0.41730356],\n",
       "       [0.66544366, 0.3345563 ],\n",
       "       [0.5974602 , 0.40253976],\n",
       "       [0.67161596, 0.32838404],\n",
       "       [0.6146416 , 0.38535833],\n",
       "       [0.6771497 , 0.32285026],\n",
       "       [0.65635496, 0.34364498],\n",
       "       [0.6859016 , 0.31409845],\n",
       "       [0.6593123 , 0.34068766],\n",
       "       [0.693511  , 0.306489  ],\n",
       "       [0.6589541 , 0.34104592],\n",
       "       [0.5904506 , 0.4095494 ],\n",
       "       [0.6568329 , 0.34316716],\n",
       "       [0.70321685, 0.29678312],\n",
       "       [0.71257186, 0.28742817],\n",
       "       [0.6958822 , 0.30411783],\n",
       "       [0.71671414, 0.2832859 ],\n",
       "       [0.7133471 , 0.28665292],\n",
       "       [0.66721004, 0.33278987],\n",
       "       [0.5540392 , 0.44596082],\n",
       "       [0.40243676, 0.5975632 ],\n",
       "       [0.55865955, 0.4413405 ],\n",
       "       [0.4155658 , 0.5844342 ],\n",
       "       [0.25921988, 0.7407801 ],\n",
       "       [0.40421155, 0.5957884 ],\n",
       "       [0.25295657, 0.74704343],\n",
       "       [0.39011553, 0.6098845 ],\n",
       "       [0.53569686, 0.46430317],\n",
       "       [0.6425222 , 0.35747784],\n",
       "       [0.54790324, 0.45209676],\n",
       "       [0.40938795, 0.59061205],\n",
       "       [0.27125573, 0.72874427],\n",
       "       [0.39824945, 0.6017506 ],\n",
       "       [0.53374165, 0.46625835],\n",
       "       [0.61545575, 0.38454416],\n",
       "       [0.60541856, 0.3945814 ],\n",
       "       [0.5517528 , 0.44824716],\n",
       "       [0.47801688, 0.521983  ],\n",
       "       [0.5410374 , 0.4589626 ],\n",
       "       [0.5902837 , 0.40971625]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 32.41864489,  28.02453063,  23.66899843,  19.02647195,\n",
       "        14.44246863,   9.58987955,   4.14167489,  -1.55991419,\n",
       "        -7.17597479, -13.04024696, -18.81479251, -24.16285759,\n",
       "       -29.41819554, -34.24074434, -38.96087529, -43.2351937 ,\n",
       "       -46.71930612, -49.73681777, -51.93575419, -53.63138811,\n",
       "       -54.46407002, -54.07090535, -52.75205264, -50.79940095,\n",
       "       -48.49855202, -45.46687502, -41.98291942, -38.32019818,\n",
       "       -34.74919824, -31.53939372, -28.96125087, -26.62672807,\n",
       "       -24.81340773, -23.8009608 , -23.20887409, -23.32614308,\n",
       "       -23.77873178, -24.1960943 , -24.2091749 , -24.11614073,\n",
       "       -24.21262939, -24.79383842, -25.48926674, -25.93072816,\n",
       "       -25.75031514, -24.57835769, -22.04143344, -17.76048586])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_discounted_rewards(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_to_one_hot(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningVariance:\n",
    "    # Keeps a running estimate of variance\n",
    "\n",
    "    def __init__(self):\n",
    "        self.m_k = None\n",
    "        self.s_k = None\n",
    "        self.k = None\n",
    "\n",
    "    def add(self, x):\n",
    "        if not self.m_k:\n",
    "            self.m_k = x\n",
    "            self.s_k = 0\n",
    "            self.k = 0\n",
    "        else:\n",
    "            old_mk = self.m_k\n",
    "            self.k += 1\n",
    "            self.m_k += (x - self.m_k) / self.k\n",
    "            self.s_k += (x - old_mk) * (x - self.m_k)\n",
    "\n",
    "    def get_variance(self):\n",
    "        return self.s_k / (self.k - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) episode length: 20 loss: -16.055, score: 0.45138707895045604, entropy: 1.000, running_var: 167.8182246374874\n",
      "20) episode length: 18 loss: -6.097, score: 32.569279816274815, entropy: 0.898, running_var: 925.7793653950125\n",
      "40) episode length: 46 loss: -19.972, score: 52.325048808348285, entropy: 0.747, running_var: 1709.1453919575197\n",
      "60) episode length: 90 loss: 31.949, score: 81.60682658729482, entropy: 0.792, running_var: 3139.137072113438\n",
      "80) episode length: 26 loss: -23.338, score: 155.5672765971031, entropy: 0.806, running_var: 12139.152541115673\n",
      "100) episode length: 55 loss: -30.661, score: 110.15726707678486, entropy: 0.591, running_var: 11143.463846172845\n",
      "120) episode length: 77 loss: -4.209, score: 114.5970697956941, entropy: 0.444, running_var: 10621.227081269271\n",
      "140) episode length: 53 loss: 1.901, score: 192.98286181721363, entropy: 0.348, running_var: 17404.399976312547\n",
      "160) episode length: 36 loss: -4.983, score: 38.376394602532514, entropy: 0.279, running_var: 16291.823468239963\n",
      "180) episode length: 30 loss: -3.303, score: 68.97583807914084, entropy: 0.270, running_var: 15450.724267535754\n",
      "200) episode length: 20 loss: -10.277, score: 39.45381239080715, entropy: 0.513, running_var: 14655.415104860169\n",
      "220) episode length: 24 loss: -3.800, score: 62.174213052140125, entropy: 0.261, running_var: 14024.218356267893\n",
      "240) episode length: 33 loss: -5.036, score: 41.094645465839506, entropy: 0.244, running_var: 13354.760403707578\n",
      "260) episode length: 42 loss: -4.392, score: 66.53568444476879, entropy: 0.259, running_var: 12723.548518762409\n",
      "280) episode length: 37 loss: -5.556, score: 79.32888603217789, entropy: 0.300, running_var: 12188.72088103837\n",
      "300) episode length: 36 loss: -6.407, score: 122.34286960292393, entropy: 0.294, running_var: 12506.045939574877\n",
      "320) episode length: 47 loss: -3.890, score: 136.39346565795924, entropy: 0.293, running_var: 12375.210936609508\n",
      "340) episode length: 61 loss: -4.990, score: 151.9118722541645, entropy: 0.271, running_var: 13184.231119614538\n",
      "360) episode length: 44 loss: -1.563, score: 110.58306690441339, entropy: 0.272, running_var: 12940.274494757956\n",
      "380) episode length: 34 loss: -7.525, score: 99.12683657916571, entropy: 0.351, running_var: 12498.05794903161\n",
      "400) episode length: 42 loss: -9.228, score: 88.52345462766672, entropy: 0.327, running_var: 12107.547038875424\n",
      "420) episode length: 112 loss: 20.702, score: 162.66420708112906, entropy: 0.352, running_var: 12067.617707079256\n",
      "440) episode length: 31 loss: -1.871, score: 80.70403362955209, entropy: 0.457, running_var: 11760.97846884159\n",
      "460) episode length: 21 loss: -5.718, score: 27.938256919265136, entropy: 0.416, running_var: 11575.942640602685\n",
      "480) episode length: 23 loss: -6.745, score: 9.610237836690802, entropy: 0.404, running_var: 11422.863239018343\n",
      "500) episode length: 23 loss: -3.827, score: 83.74538349692702, entropy: 0.089, running_var: 11312.894803470783\n",
      "520) episode length: 50 loss: -4.666, score: 93.36006963473503, entropy: 0.116, running_var: 11129.13751999355\n",
      "540) episode length: 55 loss: 5.979, score: 116.28620717023395, entropy: 0.221, running_var: 10996.00850772491\n",
      "560) episode length: 26 loss: -3.850, score: 92.25860712802505, entropy: 0.163, running_var: 10894.99277355507\n",
      "580) episode length: 59 loss: 0.018, score: 72.14488266350664, entropy: 0.065, running_var: 10679.596729760848\n",
      "600) episode length: 66 loss: 14.301, score: 139.93755913489576, entropy: 0.263, running_var: 11039.780770033985\n",
      "620) episode length: 46 loss: -0.011, score: 81.37948656665708, entropy: 0.040, running_var: 10937.412113886254\n",
      "640) episode length: 44 loss: 0.173, score: 111.3696493651955, entropy: 0.074, running_var: 11001.651083247698\n",
      "660) episode length: 27 loss: -0.163, score: 61.88818406020556, entropy: 0.048, running_var: 10863.448184354535\n",
      "680) episode length: 51 loss: -1.078, score: 97.34373484970907, entropy: 0.117, running_var: 10792.916056138043\n",
      "700) episode length: 37 loss: -0.941, score: 107.88000065572494, entropy: 0.161, running_var: 10826.051139372297\n",
      "720) episode length: 22 loss: -5.498, score: 98.61020312424272, entropy: 0.181, running_var: 12295.773108799509\n",
      "740) episode length: 64 loss: 2.498, score: 42.80048120003788, entropy: 0.241, running_var: 12112.485186675618\n",
      "760) episode length: 20 loss: -12.887, score: 66.97662855175926, entropy: 0.423, running_var: 11976.354898006144\n",
      "780) episode length: 20 loss: -12.817, score: 1.7322660464513515, entropy: 0.470, running_var: 11861.272842657681\n",
      "800) episode length: 20 loss: -8.919, score: 4.777666788198924, entropy: 0.522, running_var: 11748.318843792495\n",
      "820) episode length: 18 loss: -8.443, score: 11.292886079003644, entropy: 0.478, running_var: 11642.273733001617\n",
      "840) episode length: 22 loss: -10.877, score: 6.422612751296919, entropy: 0.482, running_var: 11543.15025359235\n",
      "860) episode length: 30 loss: -10.577, score: 10.889321586499353, entropy: 0.475, running_var: 11432.903405466755\n",
      "880) episode length: 23 loss: -11.450, score: 6.7595842965345785, entropy: 0.454, running_var: 11323.994692954355\n",
      "900) episode length: 39 loss: -3.013, score: 18.69094984148859, entropy: 0.380, running_var: 11191.167975958497\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-fe25c663e0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode_with_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdiscounted_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_discounted_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrew_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrew_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-801136ce1b00>\u001b[0m in \u001b[0;36mrun_episode_with_policy\u001b[0;34m(env, model, exploit)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexploit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2900\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5251\u001b[0m       with super(_DefaultGraphStack, self).get_controller(\n\u001b[1;32m   5252\u001b[0m           default) as g, context.graph_mode():\n\u001b[0;32m-> 5253\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5254\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5255\u001b[0m       \u001b[0;31m# If an exception is raised here it may be hiding a related exception in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mllab/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "running_variance = RunningVariance()\n",
    "goal = 500\n",
    "epsilon=1e-12\n",
    "model = get_policy_model_softmax(lr=0.01) #0.001\n",
    "print_every = 20\n",
    "loss_array = []\n",
    "discounted_rewards_sum = []\n",
    "steps = []\n",
    "scores = []\n",
    "rew_sum = 0\n",
    "gamma=0.999\n",
    "for num_episode in range(2000):\n",
    "    states, actions, rewards, episode_length, probs = run_episode_with_policy(env, model)\n",
    "    discounted_rewards = get_discounted_rewards(rewards, gamma=gamma)\n",
    "    rew_sum = rew_sum + rewards.sum()\n",
    "    probs = np.array(probs)\n",
    "    # discounted_rewards_normalized = (discounted_rewards.reshape(-1, 1) - discounted_rewards.mean())/discounted_rewards.std()\n",
    "    discounted_rewards_normalized = discounted_rewards.reshape(-1, 1)\n",
    "    for dr in discounted_rewards_normalized:\n",
    "        running_variance.add(dr[0])\n",
    "    pseudolabels = actions_to_one_hot(actions)*discounted_rewards_normalized\n",
    "    history = model.fit(states, pseudolabels, verbose=0) #batch_size=len(states)\n",
    "    loss = history.history['loss'][0]\n",
    "    loss_array.append(loss)\n",
    "    discounted_rewards_sum.append(discounted_rewards[0])\n",
    "    steps.append(len(states))\n",
    "    # score = score_model(model, env, 10)\n",
    "    entropy = np.mean(-np.sum(np.log(probs+epsilon)*probs, axis=1)/np.log(2))\n",
    "    rv = running_variance.get_variance()\n",
    "    if (num_episode%print_every) == 0:\n",
    "        score = rew_sum/print_every\n",
    "        scores.append(score)\n",
    "        print(f'{num_episode}) episode length: {len(states)} loss: {loss:.3f}, score: {score}, entropy: {entropy:.3f}, running_var: {rv}')\n",
    "        rew_sum = 0\n",
    "    if score>=goal:\n",
    "        print(f'Goal Reached in {num_episode} episodes! Final score:', score)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 2, figsize=(20,8))\n",
    "ax = ax.flatten()\n",
    "ax[0].set_title('loss normalizada')\n",
    "ax[0].plot(np.array(loss_array)/np.array(steps).reshape(-1))\n",
    "ax[1].set_title('discount_rewards_sum')\n",
    "ax[1].plot(discounted_rewards_sum)\n",
    "ax[2].set_title('steps')\n",
    "ax[2].plot(steps)\n",
    "ax[3].set_title('Score')\n",
    "ax[3].plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, reward_sum, discounted_rewards = run_episode(env, model, greedy=True)\n",
    "plot_episode(*states.T, actions, show_pos_thres=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time states_means, states_stds = get_observations_stats(env, lambda env: run_episode(env, model, greedy=True), N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time states_means, states_stds = get_observations_stats(env, lambda env: run_episode(env, model, greedy=False), N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_model(lr=0.1, max_num_episodes = 10000, episodes_batch_size = 50, training_epochs = 1, goal = 200, \n",
    "                     reset_model=True, hidden_layer_neurons = 128, verbose_period = 2, \n",
    "                     score_thres=10, states_means=None, states_stds=None,\n",
    "                     model_train=None, model_predict=None, epsilon=1e-12):\n",
    "    losses=[]\n",
    "    if model_train is None:\n",
    "        # Get model\n",
    "        model_train, model_predict = get_policy_model(env, hidden_layer_neurons, lr)\n",
    "    num_episode = 0\n",
    "    i = 0\n",
    "    \n",
    "    while num_episode < max_num_episodes:\n",
    "        # Get batch_size episodes for training\n",
    "        batch_states, batch_actions, discounted_rewards, batch_probs = get_batch_data(env, model_predict, episodes_batch_size)\n",
    "        if states_means is not None:\n",
    "            batch_states = (batch_states - states_means)/states_stds\n",
    "        # format data for NN\n",
    "        discounted_rewards = apply_baselines(discounted_rewards)\n",
    "        # discounted_rewards = np.ones((len(batch_actions), 1))*len(batch_states)/episodes_batch_size\n",
    "        actions_train = actions_to_one_hot(batch_actions)\n",
    "        hist = model_train.fit([batch_states, discounted_rewards], actions_train, \n",
    "                               batch_size=len(batch_states),\n",
    "                               epochs=training_epochs, verbose=0)\n",
    "        loss = hist.history['loss'][0]\n",
    "        losses.append(loss)\n",
    "        score = score_model(model_predict, env, score_thres)\n",
    "        if (i%verbose_period) == 0:\n",
    "            entropy = np.mean(-np.sum(np.log(batch_probs+epsilon)*batch_probs, axis=1)/np.log(2))\n",
    "            print(f'{num_episode}) episode avg_len: {len(batch_states)/episodes_batch_size} loss: {loss:.3f}, score: {score}, entropy: {entropy:.3f}')\n",
    "            # print(num_episode, np.mean(losses), score, entropy)\n",
    "        if score >= goal:\n",
    "            print(\"Solved in {} episodes!\".format(num_episode))\n",
    "            break\n",
    "        num_episode+=episodes_batch_size\n",
    "        i+=1\n",
    "    return losses, model_train, model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, model_train, model_predict = train_full_model(lr=0.001, training_epochs = 1, hidden_layer_neurons = 128, \n",
    "                                                      episodes_batch_size=1, max_num_episodes=10000, \n",
    "                                                      score_thres = 10, \n",
    "                                                      verbose_period = 10,\n",
    "                                                      #states_means=states_means, states_stds=states_stds,\n",
    "                                                      model_train=None, model_predict=None)\n",
    "#                                                       model_train=model_train, model_predict=model_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, reward_sum, discounted_rewards = run_episode(env, model_predict, greedy=True)\n",
    "plot_episode(*states.T, actions, show_pos_thres=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time states_means, states_stds = get_observations_stats(env, lambda env: run_episode(env, model_predict, greedy=True), N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time states_means, states_stds = get_observations_stats(env, lambda env: run_episode(env, model_predict, greedy=False), N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_predict.save('model_predict-lr_0.005-training_epochs_1-hidden_layer_neurons_128-batch_size_50.hdf5')\n",
    "# model_train.save('model_train-lr_0.005-training_epochs_1-hidden_layer_neurons_128-batch_size_50.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_episodes = 1\n",
    "reward_sum = 0\n",
    "model_train, model_predict = get_policy_model(env, hidden_layer_neurons, 0.01)\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Placeholders for our observations, outputs and rewards\n",
    "states = np.empty(0).reshape(0,dimen)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "# Setting up our environment\n",
    "observation = env.reset()\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "losses = []\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    # Append the observations to our batch\n",
    "    state = np.reshape(observation, [1, dimen])\n",
    "    \n",
    "    predict = model_predict.predict([state])[0]\n",
    "    action = np.random.choice(range(num_actions),p=predict)\n",
    "    \n",
    "    # Append the observations and outputs for learning\n",
    "    states = np.vstack([states, state])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    # Determine the oucome of our action\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    \n",
    "    if done:\n",
    "        # Determine standardized rewards\n",
    "        discounted_rewards_episode = discount_rewards(rewards, gamma)       \n",
    "        discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
    "        \n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "        if (num_episode + 1) % batch_size == 0:\n",
    "            discounted_rewards -= discounted_rewards.mean()\n",
    "            discounted_rewards /= discounted_rewards.std()\n",
    "            discounted_rewards = discounted_rewards.squeeze()\n",
    "            actions = actions.squeeze().astype(int)\n",
    "           \n",
    "            actions_train = np.zeros([len(actions), num_actions])\n",
    "            actions_train[np.arange(len(actions)), actions] = 1\n",
    "            for i in range(training_episodes):\n",
    "                loss = model_train.train_on_batch([states, discounted_rewards], actions_train)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Clear out game variables\n",
    "            states = np.empty(0).reshape(0,dimen)\n",
    "            actions = np.empty(0).reshape(0,1)\n",
    "            discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "\n",
    "        # Print periodically\n",
    "        if (num_episode + 1) % print_every == 0:\n",
    "            # Print status\n",
    "            score = score_model(model_predict,10)\n",
    "            print(\"Average reward for training episode {}: {:0.2f} Test Score: {:0.2f} Loss: {:0.6f}\".format(\n",
    "                (num_episode + 1), reward_sum/print_every, \n",
    "                score,\n",
    "                np.mean(losses[-print_every:],)\n",
    "            ))\n",
    "            \n",
    "            if score >= goal:\n",
    "                print(\"Solved in {} episodes!\".format(num_episode))\n",
    "                break\n",
    "            reward_sum = 0\n",
    "                \n",
    "        num_episode += 1\n",
    "        observation = env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
